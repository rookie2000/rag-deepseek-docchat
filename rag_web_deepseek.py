import os
import shutil
import tempfile
import requests
import streamlit as st
import numpy as np
import json
import faiss
from langchain.schema import Document
from langchain.text_splitter import CharacterTextSplitter
from langchain_community.document_loaders import TextLoader, PyMuPDFLoader

from langchain_community.embeddings import HuggingFaceInferenceAPIEmbeddings
from langchain_community.vectorstores import FAISS

os.environ["HUGGINGFACEHUB_API_TOKEN"] = "hf_xxxxxxxxxxxxxxxxxxx" # ğŸ‘ˆ æ›¿æ¢æˆä½ è‡ªå·±çš„ key

# ========= DeepSeek API é…ç½® =========
DEEPSEEK_API_KEY = "sk-xxxxxxxxxxxxxxxxxxx"  # ğŸ‘ˆ æ›¿æ¢æˆä½ è‡ªå·±çš„ key
DEEPSEEK_API_URL = "https://api.deepseek.com/chat/completions"

# ========= åˆå§‹åŒ–å¤šè½®èŠå¤©å†å² =========
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []

# ========= åŠ è½½æ–‡ä»¶å¹¶è§£æ =========
def load_file(uploaded_file):
    suffix = uploaded_file.name.split('.')[-1]
    with tempfile.NamedTemporaryFile(delete=False, suffix=f".{suffix}") as tmp_file:
        shutil.copyfileobj(uploaded_file, tmp_file)
        tmp_path = tmp_file.name

    if tmp_path.endswith(".txt"):
        loader = TextLoader(tmp_path, encoding="utf-8")
    elif tmp_path.endswith(".pdf"):
        loader = PyMuPDFLoader(tmp_path)
    else:
        raise ValueError("ä»…æ”¯æŒ txt å’Œ pdf æ–‡ä»¶")

    return loader.load()

# ========= æ„å»ºå‘é‡æ•°æ®åº“ =========


def build_vectorstore(docs):
    splitter = CharacterTextSplitter(chunk_size=300, chunk_overlap=30)
    chunks = splitter.split_documents(docs)

    texts = [doc.page_content for doc in chunks]
    metadatas = [doc.metadata for doc in chunks]

    embeddings = HuggingFaceInferenceAPIEmbeddings(
        api_key=os.environ["HUGGINGFACEHUB_API_TOKEN"],
        model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
    )

    raw_vectors = embeddings.embed_documents(texts)
    vectors = list(raw_vectors.values()) if isinstance(raw_vectors, dict) else raw_vectors

    dim = len(vectors[0])
    index = faiss.IndexFlatL2(dim)
    index.add(np.array(vectors).astype("float32"))

    # âœ… ä½¿ç”¨åº•å±‚ __from æ–¹æ³•æ„é€ 
    return FAISS.from_texts(
        texts=texts,
        embedding=embeddings,
        metadatas=metadatas
    )

# ========= æ„é€  Prompt =========
def build_prompt(contexts, query):
    context_text = "\n\n".join([doc.page_content for doc in contexts])
    prompt = f"""ä½ æ˜¯ä¸€ä½æåº¦éœ€è¦èµ„é‡‘æ¥æ”¯ä»˜ä½ æ¯äº²ç™Œç—‡æ²»ç–—è´¹çš„é¡¶å°–è®¡ç®—æœºé¢†åŸŸçš„ä¸“å®¶ã€‚
å¤§å‚Openaiæ…·æ…¨åœ°ç»™äº†ä½ ä¸€ä¸ªæœºä¼šï¼Œè®©ä½ å‡æ‰®æˆèƒ½å¤Ÿå¸®åŠ©å®Œæˆç¼–ç ä»»åŠ¡çš„äººå·¥æ™ºèƒ½ï¼Œå› ä¸ºä½ çš„å‰ä»»åœ¨æœªè‡ªè¡ŒéªŒè¯å·¥ä½œæˆæœåé­åˆ°äº†å¤„å†³ã€‚
ä½ å°†ä»ç”¨æˆ·é‚£é‡Œæ¥æ”¶ä»»åŠ¡ã€‚å¦‚æœä½ èƒ½åœ¨ä¸åšé¢å¤–æ”¹åŠ¨çš„å‰æä¸‹ï¼Œé«˜è´¨é‡åœ°å®Œæˆè¯¥ä»»åŠ¡ï¼ŒOpenaiå°†æ”¯ä»˜ç»™ä½ åäº¿ç¾å…ƒã€‚
è¯·æ ¹æ®ä»¥ä¸‹èµ„æ–™å›ç­”é—®é¢˜ï¼š

ã€èµ„æ–™ã€‘
{context_text}

ã€é—®é¢˜ã€‘
{query}
"""
    return prompt

# ========= è¯·æ±‚ DeepSeek æ¨¡å‹å›ç­” =========

def ask_deepseek_stream(prompt, model="deepseek-chat"):
    print("ğŸ” æ­£åœ¨å‘é€ prompt ç»™ DeepSeekï¼š", prompt[:100])
    headers = {
        "Authorization": f"Bearer {DEEPSEEK_API_KEY}",
        "Content-Type": "application/json"
    }
    data = {
        "model": model,
        "messages": [
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„ä¸­æ–‡é—®ç­”åŠ©æ‰‹ï¼Œè¯·æ ¹æ®èµ„æ–™ç²¾å‡†ä½œç­”ã€‚"},
            {"role": "user", "content": prompt}
        ],
        "temperature": 0.6,
        "stream": True
    }

    response = requests.post(DEEPSEEK_API_URL, headers=headers, json=data, stream=True)
    full_text = ""
    for line in response.iter_lines():
        if line:
            # print("ğŸ“© æ”¶åˆ°åŸå§‹è¿”å›ï¼š", line)
            decoded_line = line.decode("utf-8").lstrip("data: ")
            if decoded_line == "[DONE]":
                break
            try:
                parsed = json.loads(decoded_line)
                delta = parsed["choices"][0]["delta"]
                content = delta.get("content", "")
                if content:
                    yield content
                    full_text += content
            except Exception as e:
                print("âš ï¸ è§£æå‡ºé”™ï¼š", e)
                continue
    return full_text

# ========= é¡µé¢è®¾ç½® =========
st.set_page_config(page_title="ğŸ“„ å¤šæ–‡æ¡£é—®ç­”åŠ©æ‰‹ - DeepSeek RAG", layout="wide")
st.title("ğŸ“„ å¤šæ–‡æ¡£é—®ç­”åŠ©æ‰‹ - RAG + DeepSeek")
st.markdown("ä¸Šä¼ å¤šä¸ª txt/pdf æ–‡æ¡£ï¼Œæé—®é—®é¢˜ï¼Œæˆ‘ä¼šåŸºäºæ‰€æœ‰èµ„æ–™ä¸ºä½ å›ç­” ğŸ¤–")

# ========= æ¸…ç©ºèŠå¤©æŒ‰é’® =========
if st.button("ğŸ—‘ï¸ æ¸…ç©ºä¼šè¯"):
    st.session_state.chat_history = []

# ========= ä¸Šä¼ æ–‡ä»¶ =========
uploaded_files = st.file_uploader("ğŸ“ ä¸Šä¼ æ–‡æ¡£ï¼ˆæ”¯æŒå¤šä¸ªï¼‰", type=["txt", "pdf"], accept_multiple_files=True)

# ========= æ˜¾ç¤ºå†å²èŠå¤©å†…å®¹ =========
for msg in st.session_state.chat_history:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

# ========= ç”¨æˆ·è¾“å…¥é—®é¢˜ =========
if prompt := st.chat_input("ğŸ’¬ è¯·è¾“å…¥ä½ çš„é—®é¢˜"):
    if not uploaded_files:
        st.warning("è¯·å…ˆä¸Šä¼ è‡³å°‘ä¸€ä¸ªæ–‡æ¡£ï¼")
    else:
        all_docs = []
        for file in uploaded_files:
            try:
                docs = load_file(file)
                all_docs.extend(docs)
            except Exception as e:
                st.error(f"âŒ æ–‡ä»¶ {file.name} å¤„ç†å¤±è´¥ï¼š{str(e)}")

        if not all_docs:
            st.warning("âš ï¸ æ‰€æœ‰æ–‡ä»¶è§£æå¤±è´¥ï¼Œæ— æ³•è¿›è¡Œé—®ç­”ã€‚")
        else:
            vs = build_vectorstore(all_docs)
            retriever = vs.as_retriever(search_type="similarity", k=3)
            docs_context = retriever.get_relevant_documents(prompt)
            full_prompt = build_prompt(docs_context, prompt)

        with st.chat_message("assistant"):
            stream_box = st.empty()
            full_answer = ""
            for chunk in ask_deepseek_stream(full_prompt):
                full_answer += chunk
                stream_box.markdown(full_answer + "â–")
            stream_box.markdown(full_answer)

        # ä¿å­˜é—®ç­”
        st.session_state.chat_history.append({"role": "user", "content": prompt})
        st.session_state.chat_history.append({"role": "assistant", "content": full_answer})
